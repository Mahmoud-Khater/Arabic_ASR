{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-18T17:45:56.453959Z"
    },
    "id": "cNMEQG4J6KKB",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict , Audio,Dataset\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore all warnings\n",
    "from datasets import Dataset , concatenate_datasets\n",
    "import numpy as np\n",
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    pipeline\n",
    ")\n",
    "from evaluate import load\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import torchaudio\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import soundfile as sf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 2\n",
      "\n",
      "GPU 0:\n",
      "  Name: NVIDIA GeForce RTX 3060\n",
      "  Total Memory: 11.64 GB\n",
      "  Capability: (8, 6)\n",
      "\n",
      "GPU 1:\n",
      "  Name: NVIDIA GeForce RTX 3060\n",
      "  Total Memory: 11.64 GB\n",
      "  Capability: (8, 6)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    \n",
    "    for i in range(num_gpus):\n",
    "        print(f\"\\nGPU {i}:\")\n",
    "        print(f\"  Name: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Total Memory: {torch.cuda.get_device_properties(i).total_memory / (1024 ** 3):.2f} GB\")\n",
    "        print(f\"  Capability: {torch.cuda.get_device_capability(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. No GPU detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 425/425 [00:00<00:00, 6704.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['audio', 'transcription'],\n",
      "        num_rows: 425\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, Audio\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_dataset_split(split_path):\n",
    "    audio_dir = os.path.join(split_path, \"audio\")\n",
    "    text_dir = os.path.join(split_path, \"text\")\n",
    "    \n",
    "    # Get all WAV files\n",
    "    audio_files = sorted([f for f in os.listdir(audio_dir) if f.endswith(\".wav\")])\n",
    "    \n",
    "    data = []\n",
    "    for audio_file in tqdm(audio_files):\n",
    "        # Get corresponding text file\n",
    "        text_file = audio_file.replace(\".wav\", \".txt\")\n",
    "        text_path = os.path.join(text_dir, text_file)\n",
    "        \n",
    "        # Read text\n",
    "        with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            transcription = f.read().strip()\n",
    "        \n",
    "        # Add to dataset\n",
    "        data.append({\n",
    "            \"audio\": os.path.join(audio_dir, audio_file),\n",
    "            \"transcription\": transcription\n",
    "        })\n",
    "    \n",
    "    return Dataset.from_pandas(pd.DataFrame(data)).cast_column(\"audio\", Audio())\n",
    "\n",
    "# Create DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    # \"train\": create_dataset_split(\"D:\\STT_OCR_RAG\\\\train\\\\data\\\\common_voice_11_ar\\\\train\"),\n",
    "    # \"validation\": create_dataset_split(\"D:\\STT_OCR_RAG\\\\train\\\\data\\\\common_voice_11_ar\\\\valid\")\n",
    "     # \"train\": create_dataset_split(\"/media/nozom/New Volume1/egy imp data/audio_files/train\"), #linux\n",
    "    \"validation\": create_dataset_split(\"/media/nozom/New Volume1/egy imp data/dataset_split_2/valid\") #linux\n",
    "    # \"test\": create_dataset_split(\"D:\\STT_OCR_RAG\\\\train\\\\train\\\\data\\\\common_voice_11_ar\\\\test\")\n",
    "                            })\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets, DatasetDict\n",
    "\n",
    "def load_multiple_arrow_datasets(paths):\n",
    "    all_splits = {\"train\": [], \"validation\": []}\n",
    "\n",
    "    for root_path in paths:\n",
    "        for split in [\"train\", \"validation\"]:\n",
    "            split_path = os.path.join(root_path, split)\n",
    "            if not os.path.exists(split_path):\n",
    "                continue\n",
    "            chunk_dirs = sorted([\n",
    "                os.path.join(split_path, d)\n",
    "                for d in os.listdir(split_path)\n",
    "                if os.path.isdir(os.path.join(split_path, d))\n",
    "            ])\n",
    "            split_chunks = [load_from_disk(chunk_dir) for chunk_dir in chunk_dirs]\n",
    "            all_splits[split].extend(split_chunks)\n",
    "\n",
    "    # دمج كل الـ chunks في كل split\n",
    "    dataset_splits = {}\n",
    "    for split, chunks in all_splits.items():\n",
    "        if chunks:  # تأكد إن فيه بيانات\n",
    "            dataset_splits[split] = concatenate_datasets(chunks)\n",
    "\n",
    "    return DatasetDict(dataset_splits)\n",
    "\n",
    "dataset = load_multiple_arrow_datasets([\n",
    "    \"/media/nozom/New Volume1/egy imp data/data_split_3/arrow\",\n",
    "    \"/media/nozom/New Volume1/egy imp data/data_split_2/arrow\",\n",
    "    \"/media/nozom/New Volume1/egy imp data/audio_files/arrow_data\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'transcription', 'input_features', 'labels'],\n",
      "        num_rows: 16856\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['audio', 'transcription', 'input_features', 'labels'],\n",
      "        num_rows: 1689\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_model_path = \"D:/STT_OCR_RAG/models/large-v3\"\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(local_model_path)\n",
    "# processor = WhisperProcessor.from_pretrained(local_model_path, language=\"ar\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_model_path_fine = \"D:/STT_OCR_RAG/audio_transcription/voice_to_text/results_large/new/checkpoint-600\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor.save_pretrained(local_model_path_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_model_path_fine = \"D:/STT_OCR_RAG/audio_transcription/voice_to_text/results_small/checkpoint-100\"\n",
    "local_model_path_fine = \"/media/nozom/New Volume1/STT_OCR_RAG/audio_transcription/results_small/checkpoint-1140\" #linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_fine = WhisperForConditionalGeneration.from_pretrained(local_model_path_fine)\n",
    "model_fine.gradient_checkpointing_enable()\n",
    "\n",
    "processor_fine = WhisperProcessor.from_pretrained(local_model_path_fine, language=\"ar\", task=\"transcribe\")\n",
    "feature_extractor_fine = WhisperFeatureExtractor.from_pretrained(local_model_path_fine)\n",
    "tokenizer_fine = WhisperTokenizer.from_pretrained(local_model_path_fine, language=\"ar\", task=\"transcribe\")\n",
    "model_fine.generation_config.language = \"ar\"\n",
    "model_fine.generation_config.task = \"transcribe\"\n",
    "model_fine.generation_config.forced_decoder_ids = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fine.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_fine.save_pretrained(\"D:\\STT_OCR_RAG\\small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('D:\\\\STT_OCR_RAG\\\\small\\\\tokenizer_config.json',\n",
       " 'D:\\\\STT_OCR_RAG\\\\small\\\\special_tokens_map.json',\n",
       " 'D:\\\\STT_OCR_RAG\\\\small\\\\vocab.json',\n",
       " 'D:\\\\STT_OCR_RAG\\\\small\\\\merges.txt',\n",
       " 'D:\\\\STT_OCR_RAG\\\\small\\\\normalizer.json',\n",
       " 'D:\\\\STT_OCR_RAG\\\\small\\\\added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processor_fine.save_pretrained(\"D:\\STT_OCR_RAG\\small\")\n",
    "# feature_extractor_fine.save_pretrained(\"D:\\STT_OCR_RAG\\small\")\n",
    "# tokenizer_fine.save_pretrained(\"D:\\STT_OCR_RAG\\small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(local_model_path_fine)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(local_model_path_fine, language=\"ar\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_str = dataset[\"train\"][87][\"transcription\"]\n",
    "# labels = tokenizer(input_str).input_ids\n",
    "# decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "# decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "# print(f\"Input:                 {input_str}\")\n",
    "# print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "# print(f\"Decoded w/out special: {decoded_str}\")\n",
    "# print(f\"Are equal:             {input_str == decoded_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "يجي يعمل فرحه في مصر عايز يتجوز واحد يا ريتني كنت انا يا اخي والله العظيم التقرير الخطير جدا عن الجهاز المركزي للتعبئه العامه والاحصاء بحث الدخل والانس\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"validation\"]['transcription'][587])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"validation\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc94744f82024b9ca9050e10be2772a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/425 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    # Compute input features\n",
    "    batch[\"input_features\"] = feature_extractor_fine(\n",
    "        audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"]\n",
    "    ).input_features[0]\n",
    "    # Tokenize transcriptions\n",
    "    batch[\"labels\"] = tokenizer_fine(batch[\"transcription\"]).input_ids\n",
    "    return batch\n",
    "# train_subset = dataset['train'].select(range(1000))\n",
    "# validation_subset = dataset['validation'].select(range(100))\n",
    "\n",
    "# dataset = DatasetDict({\n",
    "#     'train': train_subset,\n",
    "#     'validation': validation_subset\n",
    "# })\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02WTJ21g85iU",
    "outputId": "74459f6f-3e4f-40f3-ab09-84604ee1af3a"
   },
   "outputs": [],
   "source": [
    "# model.generation_config.language = \"ar\"\n",
    "# model.generation_config.task = \"transcribe\"\n",
    "# model.generation_config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JRGCfr1K9WHP",
    "outputId": "0ebdb33f-1367-42a4-f917-9812e1c883e6"
   },
   "outputs": [],
   "source": [
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        \n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        \n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        \n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        \n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor_fine,\n",
    "    decoder_start_token_id=model_fine.config.decoder_start_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric =load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    label_ids[label_ids == -100] = tokenizer_fine.pad_token_id\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    pred_str = tokenizer_fine.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer_fine.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# فك التجميد فقط عن آخر طبقتين مثلاً:\n",
    "for name, param in model_fine.model.encoder.named_parameters():\n",
    "    if 'layers.10' in name or 'layers.11' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "VV-GDTErBXWl"
   },
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results_small/\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=10,\n",
    "    # max_steps=2000,\n",
    "    num_train_epochs=3,\n",
    "    # gradient_checkpointing=True,\n",
    "    logging_dir=\"./results_small/logs\",\n",
    "    fp16=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    predict_with_generate=True, #false\n",
    "    generation_max_length=400, #225\n",
    "    save_steps=200,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    # save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    # label_smoothing_factor=0.1,#جديد\n",
    "    \n",
    "    # lr_scheduler_type=\"cosine\"#جديد \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_features', 'labels'],\n",
       "    num_rows: 425\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "aX2Y2w_9CtX7"
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model_fine,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor_fine.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='54' max='3897' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  54/3897 16:28 < 20:17:27, 0.05 it/s, Epoch 0.04/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#66% before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fine.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wer\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m base_wer \u001b[38;5;241m=\u001b[39m compute_werV(\u001b[43mmodel\u001b[49m, dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m], processor)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseline WER before training: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_wer\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def compute_werV(model, dataset, processor, device=\"cuda\", batch_size=4):\n",
    "    \"\"\"Compute WER on validation set before training.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    # Compute input features\n",
    "       \n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        batch = dataset[i:i+batch_size]\n",
    "        \n",
    "        # Generate prediction\n",
    "        input_features = torch.stack([torch.tensor(x) for x in batch[\"input_features\"]]).to(device)\n",
    "        labels = batch[\"labels\"]  # Already tokenized\n",
    "        with torch.no_grad():\n",
    "            pred_ids = model.generate(input_features,\n",
    "                                     language=\"ar\", \n",
    "                                    task=\"transcribe\")\n",
    "        pred_texts = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        label_texts = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "        all_preds.extend(pred_texts)\n",
    "        all_labels.extend(label_texts)\n",
    "        \n",
    "    # Calculate WER\n",
    "    wer = wer_metric.compute(predictions=all_preds, references=all_labels)\n",
    "    # wer=0\n",
    "    return wer\n",
    "\n",
    "# Usage\n",
    "base_wer = compute_werV(model, dataset[\"validation\"], processor)\n",
    "print(f\"Baseline WER before training: {base_wer:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [22:02<00:00, 48.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline WER before training: 59.77%\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def compute_werV(model, dataset, processor, device=\"cuda\", batch_size=4):\n",
    "    \"\"\"Compute WER on validation set before training.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    # Compute input features\n",
    "       \n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        batch = dataset[i:i+batch_size]\n",
    "        \n",
    "        # Generate prediction\n",
    "        input_features = torch.stack([torch.tensor(x) for x in batch[\"input_features\"]]).to(device)\n",
    "        labels = batch[\"labels\"]  # Already tokenized\n",
    "        with torch.no_grad():\n",
    "            pred_ids = model.generate(input_features,\n",
    "                                     language=\"ar\", \n",
    "                                    task=\"transcribe\")\n",
    "        pred_texts = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        label_texts = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "        all_preds.extend(pred_texts)\n",
    "        all_labels.extend(label_texts)\n",
    "        \n",
    "    # Calculate WER\n",
    "    wer = wer_metric.compute(predictions=all_preds, references=all_labels)\n",
    "    # wer=0\n",
    "    return wer\n",
    "\n",
    "# Usage\n",
    "base_wer = compute_werV(model_fine, dataset[\"validation\"], processor_fine)\n",
    "print(f\"Baseline WER before training: {base_wer:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(path):\n",
    "    waveform, sr = torchaudio.load(path)\n",
    "    if waveform.shape[0] > 1:  # If multi-channel\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    if sr != 16000:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, 16000)\n",
    "    return waveform.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on Arabic audio\n",
    "audio_path = \"D:\\\\STT_OCR_RAG\\\\data\\\\12.wav\"\n",
    "audio = process_audio(audio_path)\n",
    "\n",
    "# Original model\n",
    "original_result = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")(audio, generate_kwargs={\n",
    "        \"language\": \"ar\",\n",
    "        \"task\": \"transcribe\",\n",
    "        \"return_timestamps\": True  # Enable timestamps for long audio\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model:  عشان نعرف مين اللي ورا الحدثة بتاعتنا وحسام\n",
      "\n",
      "Fine-Tuned Model: عشان نعرف مين اللي ورا الحد سابتعت أنا و حسام؟\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuned model\n",
    "peft_result = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model_fine,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "     device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")(audio,\n",
    "  generate_kwargs={\n",
    "        \"language\": \"ar\",\n",
    "        \"task\": \"transcribe\",\n",
    "        \"return_timestamps\": True  # Enable timestamps for long audio\n",
    "    })\n",
    "\n",
    "print(\"Original Model:\", original_result[\"text\"])\n",
    "print(\"\\nFine-Tuned Model:\", peft_result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6vBfVucuekK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
